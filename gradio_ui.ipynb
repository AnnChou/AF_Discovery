{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9fd1e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import pickle\n",
    "\n",
    "from glob import glob\n",
    "#import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc91e49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_output_file = \"Output/everything-val_pred_HC_pixel_sz.csv\"\n",
    "os.path.exists(RF_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ca2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## note I output all data in to a CSV everything-val_pred_HC_pixel_sz.csv, sep '|'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "RF_Output_df = pd.read_csv(RF_output_file, sep=',')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3fa3293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_a_path = \"Modelfile/rf_Arfaa_jun16.pkl\"\n",
    "os.path.exists(rf_mode_a_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61d21aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RF model\n",
    "with open(rf_model_a_path, 'rb') as file:\n",
    "    rf_model_a = pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fcc7dd0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id     filename  pixel size(mm)  head circumference (mm)  \\\n",
      "0        2   002_HC.png        0.062033                    68.75   \n",
      "1       11   011_HC.png        0.055484                    69.90   \n",
      "2     14_3  014_3HC.png        0.077308                    60.26   \n",
      "3       14   014_HC.png        0.078906                    63.34   \n",
      "4       15   015_HC.png        0.060416                    69.30   \n",
      "..     ...          ...             ...                      ...   \n",
      "295  783_4  783_4HC.png        0.246141                   320.00   \n",
      "296    787   787_HC.png        0.202475                   324.60   \n",
      "297    792   792_HC.png        0.210600                   299.17   \n",
      "298    794   794_HC.png        0.181221                   308.50   \n",
      "299    796   796_HC.png        0.257187                   335.00   \n",
      "\n",
      "              anno_filename  pixel_count  predicted HC from LM  \\\n",
      "0     002_HC_Annotation.png          861            253.375401   \n",
      "1     011_HC_Annotation.png          273            213.062696   \n",
      "2    014_3HC_Annotation.png          219            235.210190   \n",
      "3     014_HC_Annotation.png          780            268.312593   \n",
      "4     015_HC_Annotation.png          735            244.490571   \n",
      "..                      ...          ...                   ...   \n",
      "295  783_4HC_Annotation.png         1818            518.925937   \n",
      "296   787_HC_Annotation.png         1110            429.141222   \n",
      "297   792_HC_Annotation.png         1698            471.270900   \n",
      "298   794_HC_Annotation.png         1668            435.732987   \n",
      "299   796_HC_Annotation.png         1347            505.414531   \n",
      "\n",
      "     predicted HC from RF  pixel_count_a  pred HC-RF-from-Arfaa_anno  \n",
      "0                 63.5237           1537                     69.8108  \n",
      "1                 63.6042            637                     71.6494  \n",
      "2                 62.9616           1010                     66.7377  \n",
      "3                 63.7491           1635                     66.2420  \n",
      "4                 63.4824           1269                     70.3231  \n",
      "..                    ...            ...                         ...  \n",
      "295              184.0790           4108                    304.6112  \n",
      "296              142.0166           3899                    298.4695  \n",
      "297              150.3146           2669                    290.9504  \n",
      "298              126.1120           2149                    256.1271  \n",
      "299              183.5430           3088                    307.3491  \n",
      "\n",
      "[300 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "print(RF_Output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca95872f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>pixel size(mm)</th>\n",
       "      <th>head circumference (mm)</th>\n",
       "      <th>anno_filename</th>\n",
       "      <th>pixel_count</th>\n",
       "      <th>predicted HC from LM</th>\n",
       "      <th>predicted HC from RF</th>\n",
       "      <th>pixel_count_a</th>\n",
       "      <th>pred HC-RF-from-Arfaa_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>002_HC.png</td>\n",
       "      <td>0.062033</td>\n",
       "      <td>68.75</td>\n",
       "      <td>002_HC_Annotation.png</td>\n",
       "      <td>861</td>\n",
       "      <td>253.375401</td>\n",
       "      <td>63.5237</td>\n",
       "      <td>1537</td>\n",
       "      <td>69.8108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>011_HC.png</td>\n",
       "      <td>0.055484</td>\n",
       "      <td>69.90</td>\n",
       "      <td>011_HC_Annotation.png</td>\n",
       "      <td>273</td>\n",
       "      <td>213.062696</td>\n",
       "      <td>63.6042</td>\n",
       "      <td>637</td>\n",
       "      <td>71.6494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14_3</td>\n",
       "      <td>014_3HC.png</td>\n",
       "      <td>0.077308</td>\n",
       "      <td>60.26</td>\n",
       "      <td>014_3HC_Annotation.png</td>\n",
       "      <td>219</td>\n",
       "      <td>235.210190</td>\n",
       "      <td>62.9616</td>\n",
       "      <td>1010</td>\n",
       "      <td>66.7377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>014_HC.png</td>\n",
       "      <td>0.078906</td>\n",
       "      <td>63.34</td>\n",
       "      <td>014_HC_Annotation.png</td>\n",
       "      <td>780</td>\n",
       "      <td>268.312593</td>\n",
       "      <td>63.7491</td>\n",
       "      <td>1635</td>\n",
       "      <td>66.2420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>015_HC.png</td>\n",
       "      <td>0.060416</td>\n",
       "      <td>69.30</td>\n",
       "      <td>015_HC_Annotation.png</td>\n",
       "      <td>735</td>\n",
       "      <td>244.490571</td>\n",
       "      <td>63.4824</td>\n",
       "      <td>1269</td>\n",
       "      <td>70.3231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id     filename  pixel size(mm)  head circumference (mm)  \\\n",
       "0     2   002_HC.png        0.062033                    68.75   \n",
       "1    11   011_HC.png        0.055484                    69.90   \n",
       "2  14_3  014_3HC.png        0.077308                    60.26   \n",
       "3    14   014_HC.png        0.078906                    63.34   \n",
       "4    15   015_HC.png        0.060416                    69.30   \n",
       "\n",
       "            anno_filename  pixel_count  predicted HC from LM  \\\n",
       "0   002_HC_Annotation.png          861            253.375401   \n",
       "1   011_HC_Annotation.png          273            213.062696   \n",
       "2  014_3HC_Annotation.png          219            235.210190   \n",
       "3   014_HC_Annotation.png          780            268.312593   \n",
       "4   015_HC_Annotation.png          735            244.490571   \n",
       "\n",
       "   predicted HC from RF  pixel_count_a  pred HC-RF-from-Arfaa_anno  \n",
       "0               63.5237           1537                     69.8108  \n",
       "1               63.6042            637                     71.6494  \n",
       "2               62.9616           1010                     66.7377  \n",
       "3               63.7491           1635                     66.2420  \n",
       "4               63.4824           1269                     70.3231  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_Output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2518ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "# RF_Output_df.to_csv(\"Output/everything-val_pred_HC_pixel_sz.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07a2680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## function get id give head circum\n",
    "def get_headcircum_from_id(id):\n",
    "    row = RF_Output_df[RF_Output_df['id'] == id]\n",
    "    headcircum = row['head circumference (mm)'].values[0] if not row.empty else None\n",
    "    return headcircum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "697c9731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_headcircum_from_id_str(id):\n",
    "    row = RF_Output_df[RF_Output_df['id'] == str(id)]\n",
    "    headcircum = row['head circumference (mm)'].values[0] if not row.empty else None\n",
    "    return headcircum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b836acd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>pixel size(mm)</th>\n",
       "      <th>head circumference (mm)</th>\n",
       "      <th>anno_filename</th>\n",
       "      <th>pixel_count</th>\n",
       "      <th>predicted HC from LM</th>\n",
       "      <th>predicted HC from RF</th>\n",
       "      <th>pixel_count_a</th>\n",
       "      <th>pred HC-RF-from-Arfaa_anno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, filename, pixel size(mm), head circumference (mm), anno_filename, pixel_count, predicted HC from LM, predicted HC from RF, pixel_count_a, pred HC-RF-from-Arfaa_anno]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_Output_df[RF_Output_df['id']==15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e413eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.26\n"
     ]
    }
   ],
   "source": [
    "c=get_headcircum_from_id_str('14_3')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9920386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_id_from_filename(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) > 1:\n",
    "        id_str = parts[0]\n",
    "        id_str = id_str.lstrip('0')  # Remove leading zeros\n",
    "        \n",
    "        if len(parts[1]) > 1 and parts[1][0].isdigit() and parts[1][1].isalpha():\n",
    "            id_str += \"_\" + parts[1][0]\n",
    "        \n",
    "        return id_str\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb8d53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes a filename in the format nnn_HC.png\n",
    "# extracts the ID part before the underscore _, and removes any leading zeros\n",
    "def extract_id_from_filename0(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) > 1:\n",
    "        id_str = parts[0]\n",
    "        id_str = id_str.lstrip('0')  # Remove leading zeros\n",
    "        return int(id_str)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9fe5adaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14_3'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filename = \"014_3HC.png\"\n",
    "#extract_id_from_filename(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72277ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def map_id_to_filename(id):\n",
    "    id_str = str(id).zfill(3)  # Pad ID with leading zeros\n",
    "    filename = f\"{id_str}_HC.png\"\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3071de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function get filename give head circum\n",
    "def get_headcircum_from_filename(filename):\n",
    "    id = extract_id_from_filename(filename)\n",
    "    print(id)\n",
    "    row = RF_Output_df[RF_Output_df['id'] == id]\n",
    "    headcircum = row['head circumference (mm)'].values[0] if not row.empty else None\n",
    "    return headcircum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e909200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extract_id_from_filename function to the 'filename' column and update the 'id' column\n",
    "# RF_Output_df['id'] = RF_Output_df['filename'].apply(extract_id_from_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a466f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14_3\n",
      "60.26\n"
     ]
    }
   ],
   "source": [
    "#test function\n",
    "filename = \"014_3HC.png\"\n",
    "#filename ='015_HC_Annotation.png'\n",
    "#id = extract_id_from_filename(filename)\n",
    "#print(id)  # Output: 86\n",
    "\n",
    "headcircum = get_headcircum_from_filename(filename)\n",
    "print(headcircum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6812366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import os\n",
    "#import joblib\n",
    "rf_model_file =\"Modelfile/rf_seiba-Jun16.joblib\"\n",
    "os.path.exists(rf_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9fe2ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFIED - Takes Image (normal and annotation) and returns them preprocessed\n",
    "#[took out getting paths/loading images at start, and saving files at end]\n",
    "#annotation is not used? is intentional? i performed same steps to annotations\n",
    "def preprocess_images_resize_crop_gradio(image,dim):\n",
    "\n",
    "        # Crop the image according to the annotation\n",
    "        \"\"\"\n",
    "        bbox = annotation.getbbox()\n",
    "        if bbox:\n",
    "            image_cropped = image.crop(bbox)\n",
    "            ann_cropped = annotation.crop(bbox)\n",
    "        else:\n",
    "            # Handle the case where no bounding box is found\n",
    "            image_cropped = image\n",
    "            ann_cropped = annotation\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        image_gray = image.convert('L')\n",
    "        #ann_gray = annotation.convert('L')\n",
    "\n",
    "        # Apply blur\n",
    "        image_blurred = image_gray.filter(ImageFilter.BLUR)\n",
    "        #ann_blurred = ann_gray.filter(ImageFilter.BLUR)\n",
    "\n",
    "        # Augmentation\n",
    "        aug = iaa.Sequential([\n",
    "            iaa.GaussianBlur(sigma=(0, 3.0)),\n",
    "            iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "            iaa.Affine(rotate=(-45, 45)),\n",
    "            iaa.Multiply((0.8, 1.2))\n",
    "        ])\n",
    "        \n",
    "        #Crop/resize\n",
    "        image_augmented = aug.augment_image(np.array(image_blurred))\n",
    "        #ann_augmented = aug.augment_image(np.array(ann_blurred))\n",
    "\n",
    "        image_augmented = tf.convert_to_tensor(image_augmented)\n",
    "        image_augmented = tf.expand_dims(image_augmented,-1)\n",
    "        image_augmented = tf.image.resize_with_crop_or_pad(image_augmented,*dim)\n",
    "        \n",
    "        #ann_augmented = tf.convert_to_tensor(ann_augmented)\n",
    "        #ann_augmented = tf.expand_dims(ann_augmented,-1)\n",
    "        #ann_augmented = tf.image.resize_with_crop_or_pad(ann_augmented,*dim)\n",
    "        \n",
    "        #ann_augmented = tf.cast(ann_augmented,tf.float32)/255.0\n",
    "        #ann_augmented=tf.cast(ann_augmented,tf.int32)\n",
    "\n",
    "        #print(tf.shape(image_augmented), tf.shape(ann_augmented))\n",
    "        \n",
    "        return image_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4fa63586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\050_2HC.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\055_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\086_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\095_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\108_3HC.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\116_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\148_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\200_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\218_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\241_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\314_HC_prep.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\482_2HC.png\n",
      "Preprocessed image saved: Data/USimage_for_demo/preprocessed\\516_HC_prep.png\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'Data/USimage_for_demo\\\\binary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m540\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Preprocess the images and save them\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mpreprocess_images_resize_crop_gradio_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 26\u001b[0m, in \u001b[0;36mpreprocess_images_resize_crop_gradio_batch\u001b[1;34m(input_folder, output_folder, dim)\u001b[0m\n\u001b[0;32m     23\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_HC.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_HC_prep.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Load the input image\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Preprocess the image\u001b[39;00m\n\u001b[0;32m     29\u001b[0m preprocessed_image \u001b[38;5;241m=\u001b[39m preprocess_images_resize_crop_gradio(image, dim)\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\PIL\\Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'Data/USimage_for_demo\\\\binary'"
     ]
    }
   ],
   "source": [
    "\n",
    "#def preprocess_image_resize_crop_gradio(image, dim):\n",
    "#    # Resize and crop the image\n",
    "#    resized_image = image.resize(dim)\n",
    "#    cropped_image = resized_image.crop((0, 0, dim[0], dim[1]))\n",
    "\n",
    "#    return cropped_image\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_images_resize_crop_gradio_batch(input_folder, output_folder, dim):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the list of input files\n",
    "    input_files = os.listdir(input_folder)\n",
    "\n",
    "    # Preprocess each input file\n",
    "    for filename in input_files:\n",
    "        # Construct the input and output file paths\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename.replace(\"_HC.png\", \"_HC_prep.png\"))\n",
    "\n",
    "        # Load the input image\n",
    "        image = Image.open(input_path)\n",
    "\n",
    "        # Preprocess the image\n",
    "        preprocessed_image = preprocess_images_resize_crop_gradio(image, dim)\n",
    "\n",
    "        # Convert the preprocessed image to a NumPy array\n",
    "        preprocessed_array = preprocessed_image.numpy()\n",
    "\n",
    "        # Save the preprocessed image\n",
    "        preprocessed_image = Image.fromarray(preprocessed_array.squeeze(), mode=\"L\")\n",
    "        preprocessed_image.save(output_path)\n",
    "\n",
    "        print(f\"Preprocessed image saved: {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Specify the input and output directories\n",
    "input_folder = \"Data/USimage_for_demo\"\n",
    "output_folder = \"Data/USimage_for_demo/preprocessed\"\n",
    "\n",
    "# Specify the desired dimensions\n",
    "dim = (800, 540)\n",
    "\n",
    "# Preprocess the images and save them\n",
    "preprocess_images_resize_crop_gradio_batch(input_folder, output_folder, dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a3e9d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    # Flatten\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)  # Convert labels to float32\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_true_f = tf.reshape(y_true, [-1])\n",
    "    y_pred_f = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "# Register the custom loss function\n",
    "tf.keras.utils.get_custom_objects()['bce_dice_loss'] = bce_dice_loss\n",
    "\n",
    "# Register the custom metric function\n",
    "tf.keras.metrics.dice_coeff = dice_coeff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d019d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model_file = \"toIgnore/Modelfile_large/unet_custom.hdf5\"\n",
    "os.path.exists(unet_model_file)\n",
    "\n",
    "# Load the model with custom metric function\n",
    "model = tf.keras.models.load_model(unet_model_file, custom_objects={'dice_coeff': dice_coeff})\n",
    "#model = tf.keras.models.load_model(unet_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d13da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image, annotation,dim,resize_only=False):\n",
    "\n",
    "        # Convert to grayscale\n",
    "        image_gray = image.convert('L')\n",
    "        ann_gray = annotation.convert('L')\n",
    "\n",
    "        if resize_only==False:\n",
    "\n",
    "          # Apply blur\n",
    "          image_blurred = image_gray.filter(ImageFilter.BLUR)\n",
    "          ann_blurred = ann_gray.filter(ImageFilter.BLUR)\n",
    "\n",
    "          # Augmentation\n",
    "          aug = iaa.Sequential([\n",
    "              iaa.GaussianBlur(sigma=(0, 3.0)),\n",
    "              iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "              iaa.Affine(rotate=(-45, 45)),\n",
    "              iaa.Multiply((0.8, 1.2))\n",
    "          ])\n",
    "\n",
    "          #Crop/resize\n",
    "          image_augmented = aug.augment_image(np.array(image_blurred))\n",
    "          ann_augmented = aug.augment_image(np.array(ann_blurred))\n",
    "\n",
    "          image_augmented = tf.convert_to_tensor(image_augmented)\n",
    "          image_augmented = tf.expand_dims(image_augmented,-1)\n",
    "          image_augmented = tf.image.resize_with_crop_or_pad(image_augmented,*dim)\n",
    "\n",
    "          ann_augmented = tf.convert_to_tensor(ann_augmented)\n",
    "          ann_augmented = tf.expand_dims(ann_augmented,-1)\n",
    "          ann_augmented = tf.image.resize_with_crop_or_pad(ann_augmented,*dim)\n",
    "\n",
    "          ann_augmented = tf.cast(ann_augmented,tf.float32)/255.0\n",
    "          ann_augmented=tf.cast(ann_augmented,tf.int32)\n",
    "\n",
    "        else:\n",
    "\n",
    "          #Crop/resize\n",
    "          image_augmented = np.array(image_gray)\n",
    "          ann_augmented = np.array(ann_gray)\n",
    "\n",
    "          image_augmented = tf.convert_to_tensor(image_augmented)\n",
    "          image_augmented = tf.expand_dims(image_augmented,-1)\n",
    "          image_augmented = tf.image.resize_with_crop_or_pad(image_augmented,*dim)\n",
    "\n",
    "          ann_augmented = tf.convert_to_tensor(ann_augmented)\n",
    "          ann_augmented = tf.expand_dims(ann_augmented,-1)\n",
    "          ann_augmented = tf.image.resize_with_crop_or_pad(ann_augmented,*dim)\n",
    "\n",
    "          ann_augmented = tf.cast(ann_augmented,tf.float32)/255.0\n",
    "          ann_augmented=tf.cast(ann_augmented,tf.int32)\n",
    "\n",
    "        return image_augmented,ann_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d3f11c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:43: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\utils.py:951: UserWarning: Expected 2 arguments for function <function UNet_predict at 0x000002313204C790>, received 1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\utils.py:955: UserWarning: Expected at least 2 arguments for function <function UNet_predict at 0x000002313204C790>, received 1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    }
   ],
   "source": [
    "#jun 20  #2  Filename = \"NONE\"   IMAGE =black\n",
    "\n",
    "def preprocess_image_gradio(image):\n",
    "    # Resize the image to match the input size of the model\n",
    "    resized_image = image.resize((640, 640))\n",
    "\n",
    "    # Convert the image to grayscale and normalize pixel values\n",
    "    grayscale_image = resized_image.convert(\"L\")\n",
    "    normalized_image = np.array(grayscale_image) / 255.0\n",
    "    normalized_image = np.expand_dims(normalized_image, axis=-1)\n",
    "    normalized_image = np.expand_dims(normalized_image, axis=0)\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "\n",
    "def postprocess_prediction(prediction):\n",
    "    # Apply a threshold to convert the prediction probabilities to binary values\n",
    "    threshold = 0.5\n",
    "    prediction_binary = (prediction > threshold).astype(np.uint8)\n",
    "\n",
    "    # Scale the prediction values to the range of 0 to 255\n",
    "    prediction_scaled = prediction_binary * 255\n",
    "\n",
    "    return prediction_scaled\n",
    "\n",
    "\n",
    "def UNet_predict(input_image, filename):\n",
    "    # Preprocess the input image\n",
    "    image = Image.fromarray(input_image)\n",
    "    preprocessed_image = preprocess_image_gradio(image)\n",
    "\n",
    "    # Make the prediction using the model\n",
    "    prediction = model.predict(preprocessed_image)[0]\n",
    "\n",
    "    # Postprocess the prediction\n",
    "    prediction_image = postprocess_prediction(prediction)\n",
    "\n",
    "    # Reshape the prediction image to remove the extra dimension\n",
    "    prediction_image = np.squeeze(prediction_image, axis=-1)\n",
    "\n",
    "    # Create a PIL image from the prediction array\n",
    "    prediction_image = Image.fromarray(prediction_image, mode='L')\n",
    "\n",
    "    # Resize the prediction image to match the original size\n",
    "    original_size = image.size\n",
    "    prediction_image = prediction_image.resize(original_size)\n",
    "\n",
    "    # Convert the PIL image to a NumPy array\n",
    "    prediction_image = np.array(prediction_image)\n",
    "\n",
    "    # Count the number of pixels in the prediction image\n",
    "    pixel_count = np.count_nonzero(prediction_image)\n",
    "\n",
    "    # Return the predicted output as an image, the pixel count, and the filename\n",
    "    return prediction_image, str(pixel_count), str(filename)\n",
    "\n",
    "input_image = gr.inputs.Image(label=\"Input Image\", type=\"numpy\")\n",
    "\n",
    "output_image = gr.outputs.Image(label=\"Prediction\", type=\"numpy\")\n",
    "output_pixel_count = gr.outputs.Label(label=\"Pixel Count\")\n",
    "output_filename = gr.outputs.Label(label=\"Filename\")\n",
    "\n",
    "gr.Interface(fn=UNet_predict, inputs=input_image, outputs=[output_image, output_pixel_count, output_filename], title=\"HC Predictor Demo\").launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d053feab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:43: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n"
     ]
    }
   ],
   "source": [
    "#jun 20  #3  Manual ID input    IMAGE =black\n",
    "\n",
    "def preprocess_image_gradio(image):\n",
    "    # Resize the image to match the input size of the model\n",
    "    resized_image = image.resize((640, 640))\n",
    "\n",
    "    # Convert the image to grayscale and normalize pixel values\n",
    "    grayscale_image = resized_image.convert(\"L\")\n",
    "    normalized_image = np.array(grayscale_image) / 255.0\n",
    "    normalized_image = np.expand_dims(normalized_image, axis=-1)\n",
    "    normalized_image = np.expand_dims(normalized_image, axis=0)\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "\n",
    "def postprocess_prediction(prediction):\n",
    "    # Apply a threshold to convert the prediction probabilities to binary values\n",
    "    threshold = 0.5\n",
    "    prediction_binary = (prediction > threshold).astype(np.uint8)\n",
    "\n",
    "    # Scale the prediction values to the range of 0 to 255\n",
    "    prediction_scaled = prediction_binary * 255\n",
    "\n",
    "    return prediction_scaled\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def preprocess_image_gradio(image):\n",
    "    # Resize the image to match the input size of the model\n",
    "    resized_image = image.resize((640, 640))\n",
    "\n",
    "    # Convert the image to grayscale and normalize pixel values\n",
    "    grayscale_image = resized_image.convert(\"L\")\n",
    "    normalized_image = np.array(grayscale_image) / 255.0\n",
    "    normalized_image = np.expand_dims(normalized_image, axis=-1)\n",
    "    normalized_image = np.expand_dims(normalized_image, axis=0)\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "\n",
    "def postprocess_prediction(prediction):\n",
    "    # Apply a threshold to convert the prediction probabilities to binary values\n",
    "    threshold = 0.5\n",
    "    prediction_binary = (prediction > threshold).astype(np.uint8)\n",
    "\n",
    "    # Scale the prediction values to the range of 0 to 255\n",
    "    prediction_scaled = prediction_binary * 255\n",
    "\n",
    "    return prediction_scaled\n",
    "\n",
    "\n",
    "def UNet_predict(input_image, filename):\n",
    "    # Load the input image file\n",
    "    image = Image.fromarray(input_image)\n",
    "\n",
    "    # Preprocess the input image\n",
    "    preprocessed_image = preprocess_image_gradio(image)\n",
    "\n",
    "    # Make the prediction using the model\n",
    "    prediction = model.predict(preprocessed_image)[0]\n",
    "\n",
    "    # Postprocess the prediction\n",
    "    prediction_image = postprocess_prediction(prediction)\n",
    "\n",
    "    # Reshape the prediction image to remove the extra dimension\n",
    "    prediction_image = np.squeeze(prediction_image, axis=-1)\n",
    "\n",
    "    # Create a PIL image from the prediction array\n",
    "    prediction_image = Image.fromarray(prediction_image, mode='L')\n",
    "\n",
    "    # Resize the prediction image to match the original size\n",
    "    original_size = image.size\n",
    "    prediction_image = prediction_image.resize(original_size)\n",
    "\n",
    "    # Convert the PIL image to a NumPy array\n",
    "    prediction_image = np.array(prediction_image)\n",
    "\n",
    "    # Count the number of pixels in the prediction image\n",
    "    pixel_count = np.count_nonzero(prediction_image)\n",
    "\n",
    "    # Return the predicted output as an image, the pixel count, and the filename\n",
    "    return prediction_image, str(pixel_count), filename\n",
    "\n",
    "\n",
    "input_image = gr.inputs.Image(label=\"Input Image\", type=\"numpy\")\n",
    "input_filename = gr.inputs.Textbox(label=\"Filename\")\n",
    "output_image = gr.outputs.Image(label=\"Prediction\", type=\"numpy\")\n",
    "output_pixel_count = gr.outputs.Label(label=\"Pixel Count\")\n",
    "output_filename = gr.outputs.Label(label=\"Filename\")\n",
    "\n",
    "gr.Interface(fn=UNet_predict, inputs=[input_image, input_filename], outputs=[output_image, output_pixel_count, output_filename], title=\"HC Predictor Demo\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b4b0975e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "image_path = 'Img/QR_binary_gradio.png'\n",
    "Image(filename=image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b0becff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:59: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7873\n",
      "Running on public URL: https://ddab7a3351f72a9623.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ddab7a3351f72a9623.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### put my last binary image random forest regression here  ####\n",
    "\n",
    "\n",
    "# Load the RF model\n",
    "\n",
    "\n",
    "def get_pixel_count_gradio(input_img):\n",
    "    # Add your implementation to calculate the pixel count from the input image\n",
    "    pixel_count = 0  # Replace with your own code\n",
    "    return pixel_count\n",
    "\n",
    "def predict_head_circumference(input_img, pixel_size):\n",
    "    # Use the pixel size and other features to make a prediction\n",
    "    feature_names = ['pixel_count', 'pixel size(mm)']  # Add other relevant features here\n",
    "    pixel_count = get_pixel_count_gradio(input_img)\n",
    "    \n",
    "    #prediction = rf_model.predict([[pixel_count, pixel_size]])[0]\n",
    "    prediction = rf_model_a.predict([[pixel_count, pixel_size]])[0]\n",
    "\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "inputs = [\n",
    "    gr.inputs.Image(label=\"Image\"),\n",
    "    gr.inputs.Number(label=\"Pixel Size (mm)\")\n",
    "]\n",
    "\n",
    "output = gr.outputs.Textbox(label=\"Head Circumference (mm)\")\n",
    "\n",
    "rf_interface = gr.Interface(\n",
    "    fn=predict_head_circumference,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    title=\"Head Circumference Predictor\",\n",
    "    description=\"Predicts the head circumference based on an image and pixel size.\",\n",
    ")\n",
    "\n",
    "rf_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe91b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "832fd6a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'fn', 'inputs', and 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m gr\u001b[38;5;241m.\u001b[39mInterface(fn\u001b[38;5;241m=\u001b[39mget_headcircum_from_filename_gradio,  inputs\u001b[38;5;241m=\u001b[39minput_image, outputs\u001b[38;5;241m=\u001b[39moutput_id )\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m#gr.Interface(fn=predict, inputs=input_image, outputs=[output_image, output_pixel_count, output_headcircum], title=title).launch()\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInterface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'fn', 'inputs', and 'outputs'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Function to get head circumference from the filename\n",
    "# def get_headcircum_from_filename(filename): - see box above\n",
    "def get_headcircum_from_filename_gradio (filename):\n",
    "    id = extract_id_from_filename(filename)\n",
    "    print(id)\n",
    "    row = RF_Output_df[RF_Output_df['id'] == id]\n",
    "    headcircum = row['head circumference (mm)'].values[0] if not row.empty else None\n",
    "    return id, headcircum\n",
    "\n",
    "\n",
    "# Modify the predict function\n",
    "def predict(image, filename):\n",
    "    # Preprocess the input image\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "\n",
    "    # Make the prediction using the model\n",
    "    prediction = model.predict(preprocessed_image)[0]\n",
    "\n",
    "    # Calculate the pixel count\n",
    "    pixel_count = np.sum(prediction > 0.5)  # Adjust the threshold as needed\n",
    "\n",
    "    # Get the head circumference from the filename\n",
    "    headcircum = get_headcircum_from_filename(filename)\n",
    "\n",
    "    # Return the predicted output, pixel count, and head circumference\n",
    "    return prediction, pixel_count, headcircum\n",
    "\n",
    "# Define the inputs and outputs\n",
    "input_image = gr.inputs.Image(label=\"Input Image\", type=\"pil\")\n",
    "output_image = gr.outputs.Image(label=\"Prediction\", type=\"numpy\")\n",
    "output_pixel_count = gr.outputs.Label(label=\"Pixel Count\")\n",
    "output_headcircum = gr.outputs.Label(label=\"Head Circumference\")\n",
    "output_id = gr.outputs.Label(label=\"ID\")\n",
    "\n",
    "# Create the Gradio interface with a title\n",
    "title = \"HC Predictor 2\"\n",
    "gr.Interface(fn=get_headcircum_from_filename_gradio,  inputs=input_image, outputs=output_id )\n",
    "#gr.Interface(fn=predict, inputs=input_image, outputs=[output_image, output_pixel_count, output_headcircum], title=title).launch()\n",
    "\n",
    "gr.Interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62035551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:637: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\routes.py\", line 394, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\blocks.py\", line 1075, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\blocks.py\", line 884, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\annch\\AppData\\Local\\Temp\\ipykernel_284\\3680166849.py\", line 25, in predict\n",
      "    headcircum = get_headcircum_from_filename(filename)\n",
      "  File \"C:\\Users\\annch\\AppData\\Local\\Temp\\ipykernel_284\\677485304.py\", line 5, in get_headcircum_from_filename\n",
      "    id = extract_id_from_filename(filename)\n",
      "  File \"C:\\Users\\annch\\AppData\\Local\\Temp\\ipykernel_284\\3503621901.py\", line 2, in extract_id_from_filename\n",
      "    parts = filename.split('_')\n",
      "AttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_images(image):\n",
    "    # Define the desired image dimensions for preprocessing\n",
    "    dim = (224, 224)\n",
    "    \n",
    "    # Convert the input image to grayscale\n",
    "    image_gray = image.convert('L')\n",
    "    \n",
    "    # Apply blur\n",
    "    image_blurred = image_gray.filter(ImageFilter.BLUR)\n",
    "\n",
    "    # Augmentation\n",
    "    aug = iaa.Sequential([\n",
    "        iaa.GaussianBlur(sigma=(0, 3.0)),\n",
    "        iaa.AdditiveGaussianNoise(scale=(0, 0.05 * 255)),\n",
    "        iaa.Affine(rotate=(-45, 45)),\n",
    "        iaa.Multiply((0.8, 1.2))\n",
    "    ])\n",
    "\n",
    "    # Convert the augmented image to a tensor and resize it\n",
    "    image_augmented = np.array(image_blurred)\n",
    "    image_augmented = aug.augment_image(image_augmented)\n",
    "    image_augmented = tf.convert_to_tensor(image_augmented)\n",
    "    image_augmented = tf.expand_dims(image_augmented, -1)\n",
    "    image_augmented = tf.image.resize_with_crop_or_pad(image_augmented, *dim)\n",
    "    image_augmented = tf.cast(image_augmented, tf.float32) / 255.0\n",
    "\n",
    "    return image_augmented\n",
    "\n",
    "def predict(image):\n",
    "    # Preprocess the input image\n",
    "    preprocessed_image = preprocess_images(image)\n",
    "\n",
    "    # Make the prediction\n",
    "    prediction = model.predict(tf.expand_dims(preprocessed_image, axis=0))[0]\n",
    "\n",
    "    # Return the predicted output\n",
    "    return prediction\n",
    "\n",
    "input_image = gr.inputs.Image(label=\"Input Image\")\n",
    "output_prediction = gr.outputs.Label(label=\"Prediction\")\n",
    "\n",
    "gr.Interface(fn=predict, inputs=input_image, outputs=output_prediction).launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f57d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fbf9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RF model\n",
    "rf_model = joblib.load(rf_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019cc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01bceddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:59: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "Running on public URL: https://d7fa423565ba291b59.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d7fa423565ba291b59.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "#import joblib\n",
    "\n",
    "# Load the RF model\n",
    "#rf_model = joblib.load(\"path/to/rf_model.joblib\")\n",
    "\n",
    "def get_pixel_count_gradio(input_img):\n",
    "    # Add your implementation to calculate the pixel count from the input image\n",
    "    pixel_count = 0  # Replace with your own code\n",
    "    return pixel_count\n",
    "\n",
    "def predict_head_circumference_b1(input_img, pixel_size):\n",
    "    # Use the pixel size and other features to make a prediction\n",
    "    feature_names = ['pixel_count', 'pixel size(mm)']  # Add other relevant features here\n",
    "    pixel_count = get_pixel_count_gradio(input_img)\n",
    "    \n",
    "    prediction = rf_model.predict([[pixel_count, pixel_size]])[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "inputs = [\n",
    "    gr.inputs.Image(label=\"Image\"),\n",
    "    gr.inputs.Number(label=\"Pixel Size (mm)\")\n",
    "]\n",
    "\n",
    "output = gr.outputs.Textbox(label=\"Head Circumference (mm)\")\n",
    "\n",
    "gr_interface = gr.Interface(\n",
    "    fn=predict_head_circumference_b1,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    title=\"Head Circumference Predictor (binary image)\",\n",
    "    description=\"Predicts the head circumference based on an image and pixel size.\",\n",
    ")\n",
    "\n",
    "gr_interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1c0591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:59: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "Running on public URL: https://418536eb638c3497af.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://418536eb638c3497af.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pixel_count_gradio(input_img):\n",
    "    # Convert the input image to grayscale\n",
    "    grayscale_img = cv2.cvtColor(input_img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Calculate the pixel count by counting the non-zero pixels\n",
    "    pixel_count = np.count_nonzero(grayscale_img)\n",
    "    \n",
    "    return pixel_count\n",
    "\n",
    "def predict_head_circumference_b1(input_img, pixel_size):\n",
    "    # Use the pixel size and other features to make a prediction\n",
    "    feature_names = ['pixel_count', 'pixel size(mm)']  # Add other relevant features here\n",
    "    pixel_count = get_pixel_count_gradio(input_img)\n",
    "    \n",
    "    prediction = rf_model.predict([[pixel_count, pixel_size]])[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "inputs = [\n",
    "    gr.inputs.Image(label=\"Image\"),\n",
    "    gr.inputs.Number(label=\"Pixel Size (mm)\")\n",
    "]\n",
    "\n",
    "output = gr.outputs.Textbox(label=\"Head Circumference (mm)\")\n",
    "\n",
    "gr_interface = gr.Interface(\n",
    "    fn=predict_head_circumference_b1,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    title=\"Head Circumference Predictor (Ultrasound image)\",\n",
    "    description=\"Predicts the head circumference based on an image and pixel size.\",\n",
    ")\n",
    "\n",
    "gr_interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa578321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:59: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7868\n",
      "Running on public URL: https://3bc0b762991056740a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3bc0b762991056740a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Not working\n",
    "def get_pixel_count_gradio(input_img):\n",
    "    # Convert the input image to grayscale\n",
    "    grayscale_img = cv2.cvtColor(input_img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Calculate the pixel count by counting the non-zero pixels\n",
    "    pixel_count = np.count_nonzero(grayscale_img)\n",
    "    \n",
    "    return pixel_count\n",
    "\n",
    "def predict_head_circumference_us(input_img, pixel_size):\n",
    "    # Use the pixel size and other features to make a prediction\n",
    "    feature_names = ['pixel_count', 'pixel size(mm)', 'numpy']  # Add other relevant features here\n",
    "    pixel_count = get_pixel_count_gradio(input_img)\n",
    "    \n",
    "    # Read the filename from the input image\n",
    "    filename = input_img.name\n",
    "    \n",
    "    prediction = rf_model.predict([[pixel_count, pixel_size, filename]])[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "inputs = [\n",
    "    gr.inputs.Image(label=\"Image\", type='numpy'),\n",
    "    gr.inputs.Number(label=\"Pixel Size (mm)\")\n",
    "]\n",
    "\n",
    "output = gr.outputs.Textbox(label=\"Head Circumference (mm)\")\n",
    "\n",
    "gr_interface = gr.Interface(\n",
    "    fn=predict_head_circumference_b1,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    title=\"Head Circumference Predictor\",\n",
    "    description=\"Predicts the head circumference based on an image and pixel size.\",\n",
    ")\n",
    "\n",
    "gr_interface.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac51f47d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid ID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create a Gradio interface\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m iface \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_head_circumference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHead Circumference Lookup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter an ID and upload an image to retrieve the corresponding head circumference\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID123\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/image.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_flagging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Launch the Gradio app\u001b[39;00m\n\u001b[0;32m     24\u001b[0m iface\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\interface.py:475\u001b[0m, in \u001b[0;36mInterface.__init__\u001b[1;34m(self, fn, inputs, outputs, examples, cache_examples, examples_per_page, live, interpretation, num_shap, title, description, article, thumbnail, theme, css, allow_flagging, flagging_options, flagging_dir, flagging_callback, analytics_enabled, batch, max_batch_size, _api_mode, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattach_interpretation_events(\n\u001b[0;32m    468\u001b[0m         interpretation_btn,\n\u001b[0;32m    469\u001b[0m         interpretation_set,\n\u001b[0;32m    470\u001b[0m         input_component_column,\n\u001b[0;32m    471\u001b[0m         interpret_component_column,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattach_flagging_events(flag_btns, clear_btn)\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_article()\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_file()\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\interface.py:791\u001b[0m, in \u001b[0;36mInterface.render_examples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m non_state_inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    786\u001b[0m     c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_components \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, State)\n\u001b[0;32m    787\u001b[0m ]\n\u001b[0;32m    788\u001b[0m non_state_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    789\u001b[0m     c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_components \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, State)\n\u001b[0;32m    790\u001b[0m ]\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples_handler \u001b[38;5;241m=\u001b[39m \u001b[43mExamples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_state_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_state_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples_per_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples_per_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_api_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:54\u001b[0m, in \u001b[0;36mcreate_examples\u001b[1;34m(examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, batch)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_examples\u001b[39m(\n\u001b[0;32m     39\u001b[0m     examples: List[Any] \u001b[38;5;241m|\u001b[39m List[List[Any]] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     40\u001b[0m     inputs: IOComponent \u001b[38;5;241m|\u001b[39m List[IOComponent],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     batch: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m ):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124;03m\"\"\"Top-level synchronous function that creates Examples. Provided for backwards compatibility, i.e. so that gr.Examples(...) can be used to create the Examples component.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     examples_obj \u001b[38;5;241m=\u001b[39m \u001b[43mExamples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples_per_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples_per_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_api_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_api_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43melem_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melem_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_on_click\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_on_click\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_initiated_directly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     utils\u001b[38;5;241m.\u001b[39msynchronize_async(examples_obj\u001b[38;5;241m.\u001b[39mcreate)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples_obj\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:201\u001b[0m, in \u001b[0;36mExamples.__init__\u001b[1;34m(self, examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, batch, _initiated_directly)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mset_directory(working_directory):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    202\u001b[0m         [\n\u001b[0;32m    203\u001b[0m             component\u001b[38;5;241m.\u001b[39mpostprocess(sample)\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m component, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, example)\n\u001b[0;32m    205\u001b[0m         ]\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    207\u001b[0m     ]\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_none_processed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m     [ex \u001b[38;5;28;01mfor\u001b[39;00m (ex, keep) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(example, input_has_examples) \u001b[38;5;28;01mif\u001b[39;00m keep]\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples\n\u001b[0;32m    211\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_examples:\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mset_directory(working_directory):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 202\u001b[0m         [\n\u001b[0;32m    203\u001b[0m             component\u001b[38;5;241m.\u001b[39mpostprocess(sample)\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m component, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, example)\n\u001b[0;32m    205\u001b[0m         ]\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    207\u001b[0m     ]\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_none_processed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m     [ex \u001b[38;5;28;01mfor\u001b[39;00m (ex, keep) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(example, input_has_examples) \u001b[38;5;28;01mif\u001b[39;00m keep]\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples\n\u001b[0;32m    211\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_examples:\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:203\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mset_directory(working_directory):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    202\u001b[0m         [\n\u001b[1;32m--> 203\u001b[0m             \u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m component, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, example)\n\u001b[0;32m    205\u001b[0m         ]\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    207\u001b[0m     ]\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_none_processed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m     [ex \u001b[38;5;28;01mfor\u001b[39;00m (ex, keep) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(example, input_has_examples) \u001b[38;5;28;01mif\u001b[39;00m keep]\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples\n\u001b[0;32m    211\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_examples:\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\components.py:1634\u001b[0m, in \u001b[0;36mImage.postprocess\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processing_utils\u001b[38;5;241m.\u001b[39mencode_pil_to_base64(y)\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[1;32m-> 1634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocessing_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_url_or_file_to_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot process this value as an Image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\processing_utils.py:70\u001b[0m, in \u001b[0;36mencode_url_or_file_to_base64\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encode_url_to_base64(path)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_file_to_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\processing_utils.py:94\u001b[0m, in \u001b[0;36mencode_file_to_base64\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_file_to_base64\u001b[39m(f):\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     95\u001b[0m         encoded_string \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(file\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m     96\u001b[0m         base64_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(encoded_string, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/image.jpg'"
     ]
    }
   ],
   "source": [
    "# Function to retrieve head circumference based on ID\n",
    "def get_head_circumference(id, image):\n",
    "    try:\n",
    "        # Find the row with the given ID\n",
    "        row = df[df['ID'] == id]\n",
    "        # Retrieve the head circumference value\n",
    "        head_circumference = row['Head Circumference'].values[0]\n",
    "        return f\"Head Circumference for ID {id}: {head_circumference} cm\"\n",
    "    except:\n",
    "        return \"Invalid ID\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_head_circumference,\n",
    "    inputs=[\"text\", \"image\"],\n",
    "    outputs=\"text\",\n",
    "    title=\"Head Circumference Lookup\",\n",
    "    description=\"Enter an ID and upload an image to retrieve the corresponding head circumference\",\n",
    "    examples=[['ID123', 'path/to/image.jpg']],\n",
    "    allow_flagging=False\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc8dba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\interface.py:333: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'never'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path/to/image.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid ID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create a Gradio interface\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m iface \u001b[38;5;241m=\u001b[39m \u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInterface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_head_circumference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHead Circumference Lookup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter an ID and upload an image to retrieve the corresponding head circumference\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID123\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/image.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_flagging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Launch the Gradio app\u001b[39;00m\n\u001b[0;32m     24\u001b[0m iface\u001b[38;5;241m.\u001b[39mlaunch()\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\interface.py:475\u001b[0m, in \u001b[0;36mInterface.__init__\u001b[1;34m(self, fn, inputs, outputs, examples, cache_examples, examples_per_page, live, interpretation, num_shap, title, description, article, thumbnail, theme, css, allow_flagging, flagging_options, flagging_dir, flagging_callback, analytics_enabled, batch, max_batch_size, _api_mode, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattach_interpretation_events(\n\u001b[0;32m    468\u001b[0m         interpretation_btn,\n\u001b[0;32m    469\u001b[0m         interpretation_set,\n\u001b[0;32m    470\u001b[0m         input_component_column,\n\u001b[0;32m    471\u001b[0m         interpret_component_column,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattach_flagging_events(flag_btns, clear_btn)\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_article()\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_config_file()\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\interface.py:791\u001b[0m, in \u001b[0;36mInterface.render_examples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    785\u001b[0m non_state_inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    786\u001b[0m     c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_components \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, State)\n\u001b[0;32m    787\u001b[0m ]\n\u001b[0;32m    788\u001b[0m non_state_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    789\u001b[0m     c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_components \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, State)\n\u001b[0;32m    790\u001b[0m ]\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexamples_handler \u001b[38;5;241m=\u001b[39m \u001b[43mExamples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_state_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnon_state_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexamples_per_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples_per_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_api_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:54\u001b[0m, in \u001b[0;36mcreate_examples\u001b[1;34m(examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, batch)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_examples\u001b[39m(\n\u001b[0;32m     39\u001b[0m     examples: List[Any] \u001b[38;5;241m|\u001b[39m List[List[Any]] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     40\u001b[0m     inputs: IOComponent \u001b[38;5;241m|\u001b[39m List[IOComponent],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     batch: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m ):\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124;03m\"\"\"Top-level synchronous function that creates Examples. Provided for backwards compatibility, i.e. so that gr.Examples(...) can be used to create the Examples component.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     examples_obj \u001b[38;5;241m=\u001b[39m \u001b[43mExamples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples_per_page\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexamples_per_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_api_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_api_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43melem_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43melem_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_on_click\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_on_click\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_initiated_directly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     utils\u001b[38;5;241m.\u001b[39msynchronize_async(examples_obj\u001b[38;5;241m.\u001b[39mcreate)\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m examples_obj\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:201\u001b[0m, in \u001b[0;36mExamples.__init__\u001b[1;34m(self, examples, inputs, outputs, fn, cache_examples, examples_per_page, _api_mode, label, elem_id, run_on_click, preprocess, postprocess, batch, _initiated_directly)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mset_directory(working_directory):\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    202\u001b[0m         [\n\u001b[0;32m    203\u001b[0m             component\u001b[38;5;241m.\u001b[39mpostprocess(sample)\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m component, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, example)\n\u001b[0;32m    205\u001b[0m         ]\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    207\u001b[0m     ]\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_none_processed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m     [ex \u001b[38;5;28;01mfor\u001b[39;00m (ex, keep) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(example, input_has_examples) \u001b[38;5;28;01mif\u001b[39;00m keep]\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples\n\u001b[0;32m    211\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_examples:\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:202\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mset_directory(working_directory):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 202\u001b[0m         [\n\u001b[0;32m    203\u001b[0m             component\u001b[38;5;241m.\u001b[39mpostprocess(sample)\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m component, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, example)\n\u001b[0;32m    205\u001b[0m         ]\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    207\u001b[0m     ]\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_none_processed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m     [ex \u001b[38;5;28;01mfor\u001b[39;00m (ex, keep) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(example, input_has_examples) \u001b[38;5;28;01mif\u001b[39;00m keep]\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples\n\u001b[0;32m    211\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_examples:\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\helpers.py:203\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mset_directory(working_directory):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    202\u001b[0m         [\n\u001b[1;32m--> 203\u001b[0m             \u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m component, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs, example)\n\u001b[0;32m    205\u001b[0m         ]\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m examples\n\u001b[0;32m    207\u001b[0m     ]\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_none_processed_examples \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    209\u001b[0m     [ex \u001b[38;5;28;01mfor\u001b[39;00m (ex, keep) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(example, input_has_examples) \u001b[38;5;28;01mif\u001b[39;00m keep]\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_examples\n\u001b[0;32m    211\u001b[0m ]\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_examples:\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\components.py:1634\u001b[0m, in \u001b[0;36mImage.postprocess\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processing_utils\u001b[38;5;241m.\u001b[39mencode_pil_to_base64(y)\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[1;32m-> 1634\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocessing_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_url_or_file_to_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot process this value as an Image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\processing_utils.py:70\u001b[0m, in \u001b[0;36mencode_url_or_file_to_base64\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encode_url_to_base64(path)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_file_to_base64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\processing_utils.py:94\u001b[0m, in \u001b[0;36mencode_file_to_base64\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_file_to_base64\u001b[39m(f):\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     95\u001b[0m         encoded_string \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(file\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m     96\u001b[0m         base64_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(encoded_string, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/to/image.jpg'"
     ]
    }
   ],
   "source": [
    "# Function to retrieve head circumference based on ID\n",
    "def get_head_circumference(id, image):\n",
    "    try:\n",
    "        # Find the row with the given ID\n",
    "        row = df[df['ID'] == id]\n",
    "        # Retrieve the head circumference value\n",
    "        head_circumference = row['Head Circumference'].values[0]\n",
    "        return f\"Head Circumference for ID {id}: {head_circumference} cm\"\n",
    "    except:\n",
    "        return \"Invalid ID\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_head_circumference,\n",
    "    inputs=[\"text\", \"image\"],\n",
    "    outputs=\"text\",\n",
    "    title=\"Head Circumference Lookup\",\n",
    "    description=\"Enter an ID and upload an image to retrieve the corresponding head circumference\",\n",
    "    examples=[['ID123', 'path/to/image.jpg']],\n",
    "    allow_flagging=False\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8948258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050_\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#import gradio as gr\n",
    "#import pandas as pd\n",
    "\n",
    "# Function to retrieve head circumference based on ID\n",
    "def get_head_circumference_gradio2(filename, image):\n",
    "    try:\n",
    "        # Extract the ID from the filename\n",
    "        id = os.path.splitext(filename)[0]\n",
    "        print(id)\n",
    "        \n",
    "        # Find the row with the extracted ID\n",
    "        row = df[df['ID'] == id]\n",
    "        \n",
    "        # Retrieve the head circumference value\n",
    "        head_circumference = row['Head Circumference'].values[0]\n",
    "        \n",
    "        return f\"Head Circumference for ID {id}: {head_circumference} cm\"\n",
    "    except:\n",
    "        return \"Invalid ID\"\n",
    "\n",
    "# Create a Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=get_head_circumference_gradio2,\n",
    "    inputs=[\"text\", \"image\"],\n",
    "    outputs=\"text\",\n",
    "    title=\"Head Circumference Lookup\",\n",
    "    description=\"Enter a filename and upload an image to retrieve the corresponding head circumference\",\n",
    "    # examples=[['086_HC.png', 'path/to/image.jpg']],\n",
    "    allow_flagging=False\n",
    ")\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13a6e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  ignore below ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "774f598d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 24-25: truncated \\uXXXX escape (1061346694.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    unet_model_path=\"toIgnore\\Modelfile_large\\unet_custom.hdf5\"\u001b[0m\n\u001b[1;37m                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 24-25: truncated \\uXXXX escape\n"
     ]
    }
   ],
   "source": [
    "unet_model_path=\"toIgnore/Modelfile_large/unet_custom.hdf5\"\n",
    "os.path.exists(unet_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dab5f844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at unet_model_path",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet_model_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\keras\\saving\\save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    232\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    233\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at unet_model_path"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"unet_model_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fca43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f4a6c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imgaug'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils_unet_a\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\Proj\\T1_AF\\utils_unet_a.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimgaug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maugmenters\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01miaa\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binary_crossentropy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imgaug'"
     ]
    }
   ],
   "source": [
    "from utils_unet_a.py import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f8fdc25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown loss function: bce_dice_loss. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the UNet model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munet_custom.hdf5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Define the image segmentation function\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msegment_head_circumference\u001b[39m(image):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Preprocess the input image\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\unet-py39\\lib\\site-packages\\keras\\utils\\generic_utils.py:769\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[1;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    767\u001b[0m     obj \u001b[38;5;241m=\u001b[39m module_objects\u001b[38;5;241m.\u001b[39mget(object_name)\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure this object is passed to the `custom_objects` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    775\u001b[0m         )\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# Classes passed by name are instantiated with no args, functions are\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;66;03m# returned as-is.\u001b[39;00m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_inspect\u001b[38;5;241m.\u001b[39misclass(obj):\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown loss function: bce_dice_loss. Please ensure this object is passed to the `custom_objects` argument. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the UNet model\n",
    "model = tf.keras.models.load_model(\"unet_custom.hdf5\")\n",
    "\n",
    "# Define the image segmentation function\n",
    "def segment_head_circumference(image):\n",
    "    # Preprocess the input image\n",
    "    processed_image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "    processed_image = processed_image / 255.0\n",
    "    processed_image = np.expand_dims(processed_image, axis=0)\n",
    "    processed_image = np.expand_dims(processed_image, axis=-1)\n",
    "\n",
    "    # Perform the segmentation\n",
    "    prediction = model.predict(processed_image)\n",
    "    prediction = np.squeeze(prediction)\n",
    "    binary_image = np.where(prediction > 0.5, 255, 0).astype(np.uint8)\n",
    "\n",
    "    return binary_image\n",
    "\n",
    "# Define the input and output interfaces for Gradio\n",
    "inputs = gr.inputs.Image(label=\"Ultrasound Image (PNG format)\")\n",
    "output = gr.outputs.Image(label=\"Segmented Head Circumference\")\n",
    "\n",
    "# Create the Gradio interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=segment_head_circumference,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    title=\"Ultrasound Image Segmentation\",\n",
    "    description=\"Segment the head circumference in ultrasound images.\",\n",
    ")\n",
    "\n",
    "# Run the Gradio app on the Hugging Face Spaces platform\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f2e387",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_path_to = 'G:/.shortcut-targets-by-id/10wVB-YsfTmmpZyUo8jBH1cDOkVWVx5N1/T1 Project/Data/'\n",
    "os.path.exists(g_path_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7fd0fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to = 'Data/Training-Splitted'\n",
    "os.path.exists(path_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "527a251b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_img2_path_to = 'Data/predicted_images2'\n",
    "os.path.exists(pred_img2_path_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c347adc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(path_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5833ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path_to = 'Modelfile/'\n",
    "os.path.exists(model_path_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc587cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_joblib_name = 'pixelCt_pixelSz_toHeadCir_rfReg.joblib'\n",
    "rf_model_joblib_file = model_path_to+rf_model_joblib_name  \n",
    "os.path.exists(rf_model_joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec40836a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Modelfile/pixelCt_pixelSz_toHeadCir_rfReg.joblib'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model_joblib_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d7adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib\n",
    "\n",
    "# Load the trained Random Forest model\n",
    "rf_model = joblib.load(rf_model_joblib_file)\n",
    "\n",
    "# to use rf_model.predict(input_data)   #input_data has column of feature_names = ['pixel_count','pixel size(mm)']\n",
    "feature_names = ['pixel_count','pixel size(mm)']\n",
    "\n",
    "# X = input_dffeature_names]\n",
    "# y_pred  = rf_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb931e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e5cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### function get_pixe_count get_pixel_count(img_data_path, filename) ###  duplicate code from above.\n",
    "def get_pixel_count(img_data_path, filename):\n",
    "    file_path = os.path.join(img_data_path, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        label = Image.open(file_path)\n",
    "        label = np.array(label)\n",
    "        label = tf.convert_to_tensor(label)\n",
    "        label = tf.cast(label, tf.float32) / 255.0\n",
    "        label = tf.cast(label, tf.int32)\n",
    "        pixel_count = np.sum(label)\n",
    "    else:\n",
    "        print(\"No such file: \" + filename)\n",
    "        pixel_count = np.nan\n",
    "\n",
    "    return pixel_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "273eec06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7877\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### get_pixel_count_gradio demo ###\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def get_pixel_count_gradio(input_img):\n",
    "    label = np.array(input_img)\n",
    "    label = tf.convert_to_tensor(label)\n",
    "    label = tf.cast(label, tf.float32) / 255.0\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    pixel_count = np.sum(label)\n",
    "    print(pixel_count)\n",
    "    return pixel_count\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=get_pixel_count_gradio,\n",
    "    inputs=gr.inputs.Image(shape=(800, 540)),\n",
    "    outputs=\"number\"\n",
    ")\n",
    "\n",
    "#demo.launch(share=True)\n",
    "demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a15a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:59: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "import joblib\n",
    "\n",
    "# Load the RF model\n",
    "#rf_model = joblib.load(\"rf_model_joblib_file\")\n",
    "\n",
    "### get_pixel_count_gradio demo ###\n",
    "def get_pixel_count_gradio(input_img):\n",
    "    label = np.array(input_img)\n",
    "    pixel_count = np.sum(label)\n",
    "    print(pixel_count)\n",
    "    return pixel_count\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=get_pixel_count_gradio,\n",
    "    inputs=gr.inputs.Image(shape=(800, 540)),\n",
    "    outputs=\"number\"\n",
    ")\n",
    "\n",
    "demo.launch()\n",
    "\n",
    "def predict_head_circumference(input_img, pixel_size):\n",
    "    # Use the pixel size and other features to make a prediction\n",
    "    feature_names = ['pixel_count', 'pixel size(mm)']  # Add other relevant features here\n",
    "    pixel_count = get_pixel_count_gradio(input_img)\n",
    "    \n",
    "    prediction = rf_model.predict([[pixel_count, pixel_size]])[0]\n",
    "    print(prediction)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "inputs = [\n",
    "    gr.inputs.Image(label=\"Image\"),\n",
    "    gr.inputs.Number(label=\"Pixel Size (mm)\")\n",
    "]\n",
    "\n",
    "output = gr.outputs.Textbox(label=\"Head Circumference (mm)\")\n",
    "\n",
    "gr_interface = gr.Interface(\n",
    "    fn=predict_head_circumference,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    title=\"Head Circumference Predictor\",\n",
    "    description=\"Predicts the head circumference based on an image and pixel size.\",\n",
    ")\n",
    "\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dbfb01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:257: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\inputs.py:59: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#import gradio as gr\n",
    "#import pandas as pd\n",
    "#import joblib\n",
    "#import cv2\n",
    "\n",
    "# Load the RF model\n",
    "rf_model = joblib.load(rf_model_joblib_file)\n",
    "\n",
    "# Load the CSV file with pixel size data\n",
    "#csv_file_path = 'pixel_size_data.csv'\n",
    "#pixel_size_data = pd.read_csv(csv_file_path)\n",
    "\n",
    "def predict_head_circumference(input_img, pixel_size):\n",
    "    # Process the image (e.g., resize, normalize, etc.)\n",
    "    #processed_image = preprocess_image(image)  #image already in binary form\n",
    "    \n",
    "    # Use the pixel size and other features to make a prediction\n",
    "    feature_names = ['pixel_count','pixel size(mm)']  # Add other relevant features here\n",
    "    pixel_count = get_pixel_count_gradio(input_img)\n",
    "    pixel_size = pixel_size\n",
    "    \n",
    "    prediction = rf_model.predict([pixel_count,pixel_size])[0]\n",
    "    print(prediction)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "#def preprocess_image(image):\n",
    "#    # Preprocess the image (e.g., resize, normalize, etc.)\n",
    "#    # Modify this function based on your specific image preprocessing requirements\n",
    "#    processed_image = cv2.resize(image, (224, 224))\n",
    "#    processed_image = processed_image / 255.0  # Normalize pixel values\n",
    "    \n",
    "#    return processed_image\n",
    "\n",
    "# Define the input and output interfaces for Gradio\n",
    "inputs = [\n",
    "    gr.inputs.Image(label=\"Image\"),\n",
    "    gr.inputs.Number(label=\"Pixel Size (mm)\")\n",
    "]\n",
    "\n",
    "output = gr.outputs.Textbox(label=\"Head Circumference (mm)\")\n",
    "\n",
    "# Create the Gradio interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=predict_head_circumference,\n",
    "    inputs=inputs,\n",
    "    outputs=output,\n",
    "    title=\"Head Circumference Predictor\",\n",
    "    description=\"Predicts the head circumference based on an image and pixel size.\",\n",
    ")\n",
    "\n",
    "# Run the Gradio app\n",
    "gr_interface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06db1ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Name Here...\"),\n",
    "    #outputs=\"text\",\n",
    "    outputs=gr.outputs.Textbox(label=\"Prediction\")\n",
    ")\n",
    "demo.launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667961cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fded7624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7876\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769, 0.189], \n",
    "        [0.349, 0.686, 0.168], \n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f23fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a9e86b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get the input shape\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_shape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3603150\n",
      "137.53759999999988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annch\\.conda\\envs\\unet-py39\\lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the input shape\n",
    "input_shape = image.size\n",
    "\n",
    "print(\"Input shape:\", input_shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
